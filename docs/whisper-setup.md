# Whisper Sprachmodell Setup (OpenAI API, Cloud)

Diese Anleitung erkl√§rt, wie du OpenAI Whisper f√ºr die Spracherkennung einrichtest und konfigurierst.

---

## Was ist Whisper?

**Whisper** ist ein hochmodernes Spracherkennungssystem von OpenAI, das √ºber die OpenAI API verf√ºgbar ist.
- ‚úÖ **Sehr hohe Genauigkeit** (besonders f√ºr Deutsch)
- ‚úÖ **Unterst√ºtzt viele Sprachen** automatisch
- ‚úÖ **Robust gegen Hintergrundger√§usche**
- ‚ö†Ô∏è **Internet-Verbindung erforderlich**
- ‚ö†Ô∏è **API-Kosten** (je nach Modell und Nutzung)
- ‚ö†Ô∏è **Netzwerk-Latenz** (Audio wird an OpenAI gesendet)

---

## 1. OpenAI API Key einrichten

### API Key erhalten

1. **Registriere dich bei OpenAI:**
   - Gehe zu https://platform.openai.com/
   - Erstelle ein Konto oder melde dich an

2. **API Key erstellen:**
   - Gehe zu https://platform.openai.com/api-keys
   - Klicke auf "Create new secret key"
   - Kopiere den Key (wird nur einmal angezeigt!)

3. **Guthaben hinzuf√ºgen:**
   - Gehe zu https://platform.openai.com/account/billing
   - F√ºge Guthaben hinzu (mindestens $5 empfohlen)

### API Key konfigurieren

√ñffne die `.env` Datei:

```bash
nano .env
```

F√ºge hinzu:

```bash
# OpenAI API Key (erforderlich)
OPENAI_API_KEY=sk-...dein-api-key-hier...

# Whisper Modell (Standard: gpt-4o-mini-transcribe)
OPENAI_MODEL_STT=gpt-4o-mini-transcribe
```

**Wichtig:** Der API Key muss mit `sk-` beginnen.

---

## 2. Verf√ºgbare Whisper-Modelle

OpenAI bietet verschiedene Whisper-Modelle mit unterschiedlichen Eigenschaften:

| Modell | Geschwindigkeit | Genauigkeit | Kosten (pro Minute) | Empfehlung |
|--------|----------------|-------------|---------------------|------------|
| **whisper-1** | Schnell | Sehr gut | $0.006 | ‚úÖ **Empfohlen** |
| **gpt-4o-mini-transcribe** | Sehr schnell | Sehr gut | $0.15 | F√ºr Echtzeit-Anwendungen |
| **gpt-4o-transcribe** | Schnell | Ausgezeichnet | $0.60 | F√ºr h√∂chste Genauigkeit |

### Modell ausw√§hlen

In der `.env` Datei:

```bash
# Standard (empfohlen f√ºr beste Balance)
OPENAI_MODEL_STT=whisper-1

# Oder f√ºr schnellere Antworten (kostet mehr)
OPENAI_MODEL_STT=gpt-4o-mini-transcribe

# Oder f√ºr h√∂chste Genauigkeit (kostet am meisten)
OPENAI_MODEL_STT=gpt-4o-transcribe
```

**Hinweis:** Die Modellnamen k√∂nnen sich √§ndern. Aktuelle Modelle findest du in der [OpenAI Dokumentation](https://platform.openai.com/docs/guides/speech-to-text).

---

## 3. Installation

Die ben√∂tigten Pakete werden automatisch mit `requirements.txt` installiert:

```bash
source .venv/bin/activate
pip install -r requirements.txt
```

Das Projekt nutzt die `openai` Python-Bibliothek, die bereits in `requirements.txt` enthalten ist.

---

## 4. Verwendung

### Normalbetrieb (Push-to-Talk)

Whisper wird automatisch verwendet, wenn kein `--vosk` Flag gesetzt ist:

```bash
source .venv/bin/activate
python -m src.main
```

### Live-Spracherkennung mit Laufband-Anzeige

```bash
# Mit Whisper (OpenAI API, Standard)
python -m src.main --live-recognition
```

Die Live-Erkennung zeigt den erkannten Text kontinuierlich auf dem OLED-Display an.

### Modus-Auswahl

Beim Start kannst du zwischen zwei Modi w√§hlen:

1. **Echo Modus:**
   - Deine Sprache wird per Whisper erkannt
   - Die Box spricht den erkannten Text wieder aus (TTS)

2. **Chatbox Modus:**
   - STT (Whisper) ‚Üí LLM ‚Üí TTS (normale Chat-Antwort)

### Modus per CLI erzwingen

```bash
python -m src.main --mode echo
python -m src.main --mode chatbox
```

---

## 5. Konfiguration

### Vollst√§ndige `.env` Beispiel-Konfiguration

```bash
# OpenAI API Key (erforderlich)
OPENAI_API_KEY=sk-...dein-api-key-hier...

# Whisper Modell (Speech-to-Text)
OPENAI_MODEL_STT=whisper-1

# Chat Modell (f√ºr Chatbox-Modus)
OPENAI_MODEL_CHAT=gpt-4o-mini

# TTS Modell (Text-to-Speech)
OPENAI_MODEL_TTS=gpt-4o-mini-tts
OPENAI_TTS_VOICE=alloy

# Optional: Vosk als Alternative (siehe vosk-setup.md)
USE_VOSK=false  # true = Vosk, false = Whisper (OpenAI)
VOSK_MODEL_PATH=models/vosk-model-de-0.22
```

### TTS Stimmen

Verf√ºgbare Stimmen f√ºr `OPENAI_TTS_VOICE`:
- `alloy` (Standard)
- `echo`
- `fable`
- `onyx`
- `nova`
- `shimmer`

---

## 6. Vergleich: Whisper vs. Vosk

| Feature | OpenAI Whisper | Vosk |
|---------|----------------|------|
| **Internet** | ‚úÖ Erforderlich | ‚ùå Nicht n√∂tig |
| **Kosten** | üí∞ API-Kosten (~$0.006/Min) | ‚úÖ Kostenlos |
| **Geschwindigkeit** | ‚ö†Ô∏è Netzwerk-Latenz (~1-3s) | ‚úÖ Sehr schnell (<0.5s) |
| **Genauigkeit** | ‚úÖ Sehr gut bis ausgezeichnet | ‚úÖ Gut bis sehr gut |
| **Datenschutz** | ‚ö†Ô∏è Audio wird an OpenAI gesendet | ‚úÖ 100% lokal |
| **Sprachen** | ‚úÖ Viele Sprachen automatisch | ‚úÖ Viele Sprachen (Modell-abh√§ngig) |
| **Hintergrundger√§usche** | ‚úÖ Sehr robust | ‚ö†Ô∏è Abh√§ngig vom Modell |
| **Modell-Gr√∂√üe** | - | 45 MB - 1.8 GB |

**Empfehlung:**
- **Whisper** f√ºr h√∂chste Genauigkeit und wenn Internet verf√ºgbar ist
- **Vosk** f√ºr lokale, schnelle Erkennung ohne Internet und ohne API-Kosten

---

## 7. Kostenkontrolle

### Kosten pro Minute Audio

- **whisper-1:** ~$0.006 pro Minute
- **gpt-4o-mini-transcribe:** ~$0.15 pro Minute
- **gpt-4o-transcribe:** ~$0.60 pro Minute

### Beispiel-Kosten

Bei durchschnittlich 2 Minuten Audio pro Tag:
- **whisper-1:** ~$0.36 pro Monat
- **gpt-4o-mini-transcribe:** ~$9 pro Monat
- **gpt-4o-transcribe:** ~$36 pro Monat

### Kosten√ºberwachung

1. **OpenAI Dashboard:**
   - Gehe zu https://platform.openai.com/usage
   - Sieh dir deine API-Nutzung an

2. **Guthaben-Limits setzen:**
   - Gehe zu https://platform.openai.com/account/billing/limits
   - Setze ein monatliches Limit

---

## 8. Troubleshooting

### Problem: API Key ung√ºltig

**Fehlermeldung:**
```
RuntimeError: OPENAI_API_KEY fehlt. Bitte in .env setzen.
```
oder
```
openai.AuthenticationError: Invalid API key
```

**L√∂sung:**
1. Pr√ºfe, ob der API Key in `.env` gesetzt ist:
   ```bash
   cat .env | grep OPENAI_API_KEY
   ```

2. Pr√ºfe, ob der Key mit `sk-` beginnt

3. Pr√ºfe, ob noch Guthaben vorhanden ist:
   - https://platform.openai.com/account/billing

4. Erstelle einen neuen API Key falls n√∂tig

### Problem: Keine Internet-Verbindung

**Fehlermeldung:**
```
openai.APIConnectionError: Connection error
```

**L√∂sung:**
1. Pr√ºfe Internet-Verbindung:
   ```bash
   ping -c 3 api.openai.com
   ```

2. Falls kein Internet verf√ºgbar, verwende Vosk:
   ```bash
   python -m src.main --live-recognition --vosk
   ```

### Problem: Modell nicht gefunden

**Fehlermeldung:**
```
openai.BadRequestError: Invalid model
```

**L√∂sung:**
1. Pr√ºfe, ob das Modell in `.env` korrekt geschrieben ist:
   ```bash
   cat .env | grep OPENAI_MODEL_STT
   ```

2. Verwende einen g√ºltigen Modellnamen:
   - `whisper-1` (empfohlen)
   - `gpt-4o-mini-transcribe`
   - `gpt-4o-transcribe`

3. Pr√ºfe die [aktuelle Dokumentation](https://platform.openai.com/docs/guides/speech-to-text) f√ºr verf√ºgbare Modelle

### Problem: Langsame Erkennung

**L√∂sung:**
- Die Latenz h√§ngt von der Internet-Verbindung ab
- Typisch: 1-3 Sekunden pro Audio-Chunk
- F√ºr schnellere Erkennung: Verwende Vosk (lokal, offline)

### Problem: Schlechte Erkennungsqualit√§t

**L√∂sung:**
- Verwende ein besseres Modell (`gpt-4o-transcribe`)
- Pr√ºfe Mikrofon-Qualit√§t
- Stelle sicher, dass Sample-Rate 16000 Hz ist
- Reduziere Hintergrundger√§usche
- Sprich klarer und lauter

---

## 9. Wechsel zwischen Whisper und Vosk

### Whisper verwenden (Standard)

```bash
# In .env
USE_VOSK=false

# Oder beim Start
python -m src.main --live-recognition
```

### Vosk verwenden (lokal, offline)

```bash
# In .env
USE_VOSK=true
VOSK_MODEL_PATH=models/vosk-model-de-0.22

# Oder beim Start
python -m src.main --live-recognition --vosk
```

---

## 10. Performance-Tipps

### F√ºr Raspberry Pi 5 (8 GB)

- ‚úÖ **Whisper API** funktioniert gut, ben√∂tigt aber stabile Internet-Verbindung
- ‚úÖ **Latenz:** Typisch 1-3 Sekunden pro Erkennung
- ‚úÖ **Chunk-Dauer:** 2 Sekunden ist ein guter Kompromiss

### Optimierung

In `src/speech_recognition_live.py`:

```python
self.chunk_duration = 2.0  # Sekunden pro Chunk
# F√ºr schnellere Updates: 1.5
# F√ºr bessere Genauigkeit: 3.0
```

---

## 11. Weitere Informationen

- **OpenAI Dokumentation:** https://platform.openai.com/docs/guides/speech-to-text
- **Whisper Paper:** https://arxiv.org/abs/2212.04356
- **API Preise:** https://openai.com/api/pricing/
- **API Status:** https://status.openai.com/

---

## 12. Sicherheit & Datenschutz

### Wichtige Hinweise

- ‚ö†Ô∏è **Audio-Daten werden an OpenAI gesendet**
- ‚ö†Ô∏è **OpenAI speichert Audio-Daten gem√§√ü ihrer Datenschutzrichtlinie**
- ‚ö†Ô∏è **F√ºr sensible Daten: Verwende Vosk (lokal, offline)**

### Datenschutz-Optionen

1. **Whisper API mit Daten-Retention deaktivieren:**
   - In der API-Anfrage: `user` Parameter verwenden (nicht in diesem Projekt implementiert)
   - Siehe OpenAI Dokumentation f√ºr Details

2. **Vosk verwenden:**
   - 100% lokal, keine Daten werden gesendet
   - Siehe `docs/vosk-setup.md`

---

**Viel Erfolg mit Whisper!** üé§üîä
